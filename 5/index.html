<!DOCTYPE html>
<html>
<head>
    <title>Project 5: Fun With Diffusion Models!</title>
</head>
<body>
    <h1>Part A</h1>
    <h2>1: The sampling loop</h2>
    <p>
        In this section, we learned about and implemented the basic sampling loop used by diffusion models. The first thing we implemented was the noising 
        step used add noise to the images, which the diffusion model would then be able to remove. We also compared how iterative denoising did against 
        one-step-denoising, as well as the gaussian blur we have been familiarized with throughout the course. Through this, we can noticeably see that iterative 
        denoising performs better than the other options.
    </p>

    <figure>
        <img src="1/1.1/campanile_0.png", width="160">
        <img src="1/1.1/campanile_250.png", width="160">
        <img src="1/1.1/campanile_500.png", width="160">
        <img src="1/1.1/campanile_750.png", width="160">
        <figcaption>
            1.1 The noising process, with the original Campanile, as well as noise levels at t=250, 500, 750
        </figcaption>
    </figure>

    <figure>
        <img src="1/1.2/campanile_250_blur.png", width="160">
        <img src="1/1.2/campanile_500_blur.png", width="160">
        <img src="1/1.2/campanile_750_blur.png", width="160">
        <figcaption>
            1.2 Gaussian blur denoising for the previous t values.
        </figcaption>
    </figure>

    <figure>
        <img src="1/1.3/cleaned_250.png", width="160">
        <img src="1/1.3/cleaned_500.png", width="160">
        <img src="1/1.3/cleaned_750.png", width="160">
        <figcaption>
            1.3 One step denoising for the previous t values
        </figcaption>
    </figure>

    <figure>
        <img src="1/1.4/iterative_570.png", width="160">
        <img src="1/1.4/iterative_420.png", width="160">
        <img src="1/1.4/iterative_270.png", width="160">
        <img src="1/1.4/iterative_120.png", width="160">
        <figcaption>
            1.4 The iterative denoising process
        </figcaption>

        <img src="1/1.4/campanile_0.png", width="160">
        <img src="1/1.4/iterative_camp.png", width="160">
        <img src="1/1.4/one_step_camp.png", width="160">
        <img src="1/1.4/blur_camp.png", width="160">
        <figcaption>
            1.4 The comparison between the original, iteratively denoised, one step denoised, and gaussian blur
        </figcaption>
    </figure>
    <h2>2: Model Sampling</h2>
    <p>
        In addition to the previous part, we implemented a basic sampling loop to generate images from start. This is done by starting with pure noise, and 
        denoising from there based on some prompt embedding (in this case "a high quality photo"). This is also done from i_start = 0.
    </p>
    <figure>
        <img src="1/1.5/sample_0.png", width="160">
        <img src="1/1.5/sample_1.png", width="160">
        <img src="1/1.5/sample_2.png", width="160">
        <img src="1/1.5/sample_3.png", width="160">
        <img src="1/1.5/sample_4.png", width="160">
        <figcaption>
            1.5 5 samples generated from pure noise
        </figcaption>
    </figure>
    <h2>3: Classifier-Free Guidance (CFG)</h2>
    <p>
        From there, we improve the results by using the Classifier-Free Guidance technique (or CFG). This uses some unconditional prompt to improve image quality. 
        We use this unconditional prompt as well as the old prompt to generate a new noise estimate as some combination between the model outputs for the prompts. 
    </p>
    <figure>
        <img src="1/1.6/sample_cfg_0.png", width="160">
        <img src="1/1.6/sample_cfg_1.png", width="160">
        <img src="1/1.6/sample_cfg_2.png", width="160">
        <img src="1/1.6/sample_cfg_3.png", width="160">
        <img src="1/1.6/sample_cfg_4.png", width="160">

        <figcaption>
            1.6 5 samples generated using CFG
        </figcaption>
    </figure>
    <h2>4: Image-to-image translation</h2>
    <p>
        For this section, instead of generating from pure noise, we instead add varied amounts of noise to the image and denoise from there (resulting in a similar) 
        but still quite different image).
    </p>
    <figure>
        <img src="1/1.7/campanile_0.png", width="160">
        <img src="1/1.7/sample_cfg_camp_1.png", width="160">
        <img src="1/1.7/sample_cfg_camp_3.png", width="160">
        <img src="1/1.7/sample_cfg_camp_5.png", width="160">
        <img src="1/1.7/sample_cfg_camp_7.png", width="160">
        <img src="1/1.7/sample_cfg_camp_10.png", width="160">
        <img src="1/1.7/sample_cfg_camp_20.png", width="160">
        <br>
        <img src="1/1.7/fountain.png", width="160">
        <img src="1/1.7/sample_fountain_1.png", width="160">
        <img src="1/1.7/sample_fountain_3.png", width="160">
        <img src="1/1.7/sample_fountain_5.png", width="160">
        <img src="1/1.7/sample_fountain_7.png", width="160">
        <img src="1/1.7/sample_fountain_10.png", width="160">
        <img src="1/1.7/sample_fountain_20.png", width="160">
        <br>
        <img src="1/1.7/lighthouse.png", width="160">
        <img src="1/1.7/sample_lighthouse_1.png", width="160">
        <img src="1/1.7/sample_lighthouse_3.png", width="160">
        <img src="1/1.7/sample_lighthouse_5.png", width="160">
        <img src="1/1.7/sample_lighthouse_7.png", width="160">
        <img src="1/1.7/sample_lighthouse_10.png", width="160">
        <img src="1/1.7/sample_lighthouse_20.png", width="160">
        <figcaption>
            1.7.1 The original image, as well as the generated images at i_start = [1, 3, 5, 7, 10, 20]
        </figcaption>
    </figure>

    <p>We also repeat this for digital/hand drawn images, as the diffusion model is trained on natural images, so we get more "realistic" images back.</p>
    <figure>
        <img src="1/1.7/bird.png", width="160">
        <img src="1/1.7/sample_bird_1.png", width="160">
        <img src="1/1.7/sample_bird_3.png", width="160">
        <img src="1/1.7/sample_bird_5.png", width="160">
        <img src="1/1.7/sample_bird_7.png", width="160">
        <img src="1/1.7/sample_bird_10.png", width="160">
        <img src="1/1.7/sample_bird_20.png", width="160">
        <br>
        <img src="1/1.7/test1.png", width="160">
        <img src="1/1.7/sample_stickman_1.png", width="160">
        <img src="1/1.7/sample_stickman_3.png", width="160">
        <img src="1/1.7/sample_stickman_5.png", width="160">
        <img src="1/1.7/sample_stickman_7.png", width="160">
        <img src="1/1.7/sample_stickman_10.png", width="160">
        <img src="1/1.7/sample_stickman_20.png", width="160">
        <br>
        <img src="1/1.7/test2.png", width="160">
        <img src="1/1.7/sample_house_1.png", width="160">
        <img src="1/1.7/sample_house_3.png", width="160">
        <img src="1/1.7/sample_house_5.png", width="160">
        <img src="1/1.7/sample_house_7.png", width="160">
        <img src="1/1.7/sample_house_10.png", width="160">
        <img src="1/1.7/sample_house_20.png", width="160">
        <figcaption>
            1.7.2 The original image, as well as the generated images at i_start = [1, 3, 5, 7, 10, 20]
        </figcaption>
    </figure>
    <h2>5: Inpainting</h2>
    <p>
        For this section, we do a similar process to before, but we add an additional mask. In this masked region, after every dnoising step, we force the 
        image to have the same pixels as outside of the mask, thereby only allowing the diffusion model to generate inside the mask itself, while also inferring 
        what the image looks like using the outside context.
    </p>
    <figure>
        <img src="1/1.7.2/campanile_0.png", width="160">
        <img src="1/1.7.2/mask.png", width="160">
        <img src="1/1.7.2/inpaint_4.png", width="160">
        <br>
        <img src="1/1.7.2/lighthouse.png", width="160">
        <img src="1/1.7.2/lighthouse_mask.png", width="160">
        <img src="1/1.7.2/inpaint_lighthouse_4.png", width="160">
        <br>
        <img src="1/1.7.2/fountain.png", width="160">
        <img src="1/1.7.2/fountain_mask.png", width="160">
        <img src="1/1.7.2/inpaint_fountain_4.png", width="160">
        <figcaption>
            1.7.2 The original image, the mask (white is new, gray is to keep), and the inpainted image.
        </figcaption>
    </figure>

    <h2>6: Text-Conditional Image-toimage Translation</h2>
    <p>
        This does a similar process to part 4, but instead of using the default "high quality image", we instead use a different prompt on which to condition 
        the model output, to make the image look like the prompt but still resembling the original image in some way.
    </p>
    <figure>
        <img src="1/1.7.3/sample_rocket_camp_1.png", width="160">
        <img src="1/1.7.3/sample_rocket_camp_3.png", width="160">
        <img src="1/1.7.3/sample_rocket_camp_5.png", width="160">
        <img src="1/1.7.3/sample_rocket_camp_7.png", width="160">
        <img src="1/1.7.3/sample_rocket_camp_10.png", width="160">
        <img src="1/1.7.3/sample_rocket_camp_20.png", width="160">
        <img src="1/1.7.3/campanile_0.png", width="160">
        <br>
        <img src="1/1.7.3/sample_fountain_lighthouse_1.png", width="160">
        <img src="1/1.7.3/sample_fountain_lighthouse_3.png", width="160">
        <img src="1/1.7.3/sample_fountain_lighthouse_5.png", width="160">
        <img src="1/1.7.3/sample_fountain_lighthouse_7.png", width="160">
        <img src="1/1.7.3/sample_fountain_lighthouse_10.png", width="160">
        <img src="1/1.7.3/sample_fountain_lighthouse_20.png", width="160">
        <img src="1/1.7.2/fountain.png", width="160">
        <br>
        <img src="1/1.7.3/sample_clocktower_lighthouse_1.png", width="160">
        <img src="1/1.7.3/sample_clocktower_lighthouse_3.png", width="160">
        <img src="1/1.7.3/sample_clocktower_lighthouse_5.png", width="160">
        <img src="1/1.7.3/sample_clocktower_lighthouse_7.png", width="160">
        <img src="1/1.7.3/sample_clocktower_lighthouse_10.png", width="160">
        <img src="1/1.7.3/sample_clocktower_lighthouse_20.png", width="160">
        <img src="1/1.7.2/lighthouse.png", width="160">
        <figcaption>
            1.7.3 The translated images at i=[1,3,5,7,10,20]. The translations are: the campanile to a rocket, the fountain to a lighthouse, and the lighthouse to 
            a clocktower.
        </figcaption>
    </figure>
    <h2>7: Visual Anagrams</h2>
    <p>
        The general idea behind this section is that, we can produce two different noise estimates for the image. The first one is for the first prompt, on the 
        regular image. Then we can also generate a noise estimate for the flipped image using a second prompt. Then we can take the average of these 
        two estimates, to produce an image that looks somewhat like the first prompt normally, and somewhat like the second prompt when flipped.
    </p>
    <figure>
        <img src="1/1.8/test_anagram_0.png", width="160">
        <img src="1/1.8/test_anagram_0 - Copy.png", width="160">
        <figcaption>
            1.8 The test example, using "an oil painting of an old man" and "an oil painting of people around a campfire."
        </figcaption>
        <img src="1/1.8/anagram_1_3.png", width="160">
        <img src="1/1.8/anagram_1_3 - Copy.png", width="160">
        <figcaption>
            1.8 An anagram of a rocket ship and lighthouse
        </figcaption>
        <img src="1/1.8/anagram_2_4.png", width="160">
        <img src="1/1.8/anagram_2_4 - Copy.png", width="160">
        <figcaption>
            1.8 An anagram of a clocktower and water fountain
        </figcaption>
    </figure>

    <h2>8: Hybrid Images</h2>
    <p>
        This is simliar to a previous project we did, where you use the high frequencies of one image and the low frequencies of another image to produce a hybrid 
        image that looks different depending on how far you are. For this, we use a similar idea to part 7 along with the previous idea, and generate two noise estimates 
        for two different prompts. Then we use the highpass and lowpass filters (essentially just a gaussian kernel to blur, or get the difference between the original and blur), 
        to produce a noise estimate, such that the denoised image is a hybrid image of both prompts.
    </p>
    <figure>
        <img src="1/1.9/hybrid_skull_1.png", width="320">
        <img src="1/1.9/hybrid_skull_1.png", width="80">
        <figcaption>
            1.8 The test example, a hybrid of a skull and a waterfall.
        </figcaption>
        <img src="1/1.9/hybrid_mona_lisa_0.png", width="320">
        <img src="1/1.9/hybrid_mona_lisa_0.png", width="40">
        <figcaption>
            1.8 A hybrid of a snowy village as well as the mona lisa.
        </figcaption>
        <img src="1/1.9/hybrid_village_3.png", width="320">
        <img src="1/1.9/hybrid_village_3.png", width="40">
        <figcaption>
            1.8 A hybrid of a pencil and a snowy village.
        </figcaption>
    </figure>
    <h1>Part B</h1>
    <h2>1: Training a single-step denoising UNet</h2>
    <p>
        For this part, we first implemented a basic UNet. I added a few extra <strong>skip connections</strong> to it in the 2nd half of the convblocks themselves to help improve performance. 
        Where this differs from the true implementation of diffusion is that we estimate what the final image is, instead of the noise, by using the L2 distance between the 
        estimated image and true image as the loss. We also held the noise amount constant, which was at sigma=0.5, and show that it doesn't generalize well to different values.
    </p>
    <figure>
        <img src="2/1.2/noisy_0.png", width="120">
        <img src="2/1.2/noisy_0.2.png", width="120">
        <img src="2/1.2/noisy_0.4.png", width="120">
        <img src="2/1.2/noisy_0.5.png", width="120">
        <img src="2/1.2/noisy_0.6.png", width="120">
        <img src="2/1.2/noisy_0.8.png", width="120">
        <img src="2/1.2/noisy_1.0.png", width="120">
        <figcaption>
            1.2 What varied levels of noise look like for sigma=[0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]
        </figcaption>
    </figure>
    <figure>
        <img src="2/1.2/epoch_1_original_0.png", width="120">
        <img src="2/1.2/epoch_1_original_1.png", width="120">
        <img src="2/1.2/epoch_1_original_2.png", width="120">
        <img src="2/1.2/epoch_1_original_3.png", width="120">
        <img src="2/1.2/epoch_1_original_4.png", width="120">
        <img src="2/1.2/epoch_1_noisy_0.png", width="120">
        <img src="2/1.2/epoch_1_noisy_1.png", width="120">
        <img src="2/1.2/epoch_1_noisy_2.png", width="120">
        <img src="2/1.2/epoch_1_noisy_3.png", width="120">
        <img src="2/1.2/epoch_1_noisy_4.png", width="120">
        <img src="2/1.2/epoch_1_estimate_0.png", width="120">
        <img src="2/1.2/epoch_1_estimate_1.png", width="120">
        <img src="2/1.2/epoch_1_estimate_2.png", width="120">
        <img src="2/1.2/epoch_1_estimate_3.png", width="120">
        <img src="2/1.2/epoch_1_estimate_4.png", width="120">
        <figcaption>
            1.2 Denoising at epoch 1, showing the originals, the noisy, and the denoised.
        </figcaption>
    </figure>
    <figure>
        <img src="2/1.2/epoch_5_original_0.png", width="120">
        <img src="2/1.2/epoch_5_original_1.png", width="120">
        <img src="2/1.2/epoch_5_original_2.png", width="120">
        <img src="2/1.2/epoch_5_original_3.png", width="120">
        <img src="2/1.2/epoch_5_original_4.png", width="120">
        <img src="2/1.2/epoch_5_noisy_0.png", width="120">
        <img src="2/1.2/epoch_5_noisy_1.png", width="120">
        <img src="2/1.2/epoch_5_noisy_2.png", width="120">
        <img src="2/1.2/epoch_5_noisy_3.png", width="120">
        <img src="2/1.2/epoch_5_noisy_4.png", width="120">
        <img src="2/1.2/epoch_5_estimate_0.png", width="120">
        <img src="2/1.2/epoch_5_estimate_1.png", width="120">
        <img src="2/1.2/epoch_5_estimate_2.png", width="120">
        <img src="2/1.2/epoch_5_estimate_3.png", width="120">
        <img src="2/1.2/epoch_5_estimate_4.png", width="120">
        <figcaption>
            1.2 Denoising at epoch 1, showing the originals, the noisy, and the denoised.
        </figcaption>
    </figure>
    <figure>
        <img src="2/1.2/uncond.png", width="640">
        <figcaption>
            1.2 The training loss curve.
        </figcaption>
    </figure>
    <figure>
        <img src="2/1.2/noisy_0.png", width="120">
        <img src="2/1.2/noisy_0.2.png", width="120">
        <img src="2/1.2/noisy_0.4.png", width="120">
        <img src="2/1.2/noisy_0.5.png", width="120">
        <img src="2/1.2/noisy_0.6.png", width="120">
        <img src="2/1.2/noisy_0.8.png", width="120">
        <img src="2/1.2/noisy_1.0.png", width="120">
        <br>
        <img src="2/1.2/estimate_0.0.png", width="120">
        <img src="2/1.2/estimate_0.2.png", width="120">
        <img src="2/1.2/estimate_0.4.png", width="120">
        <img src="2/1.2/estimate_0.5.png", width="120">
        <img src="2/1.2/estimate_0.6.png", width="120">
        <img src="2/1.2/estimate_0.8.png", width="120">
        <img src="2/1.2/estimate_1.0.png", width="120">
        <figcaption>
            1.2 Testing on the different sigma values, which are out of distribution from the training data.
        </figcaption>
    </figure>
    <h2>Time Conditioning</h2>
    <p>
        Now we begin to use some of the ideas we saw in part a. For time conditioning, we modify the model so that it takes the current timestep 
        to condition the model output. This allows us to also do iterative denoising, so now we also estimate the amount of noise rather than the final image, 
        which is also closer to how diffusion actually works. We also insert a fully connected block into the model to process the timstep data, and I've also added an extra  
        <strong>skip connection</strong> here as well in the second linear layer.
    </p>
    <figure>
        <img src="2/2.2/time_condition.png", width="640">
        <figcaption>
            2.2 The training loss curve.
        </figcaption>
    </figure>
    <figure>
        <img src="2/2.2/time_cond_5.png", width="640">
        <img src="2/2.2/time_cond_epoch_5.gif", width="640">
        <br>
        <img src="2/2.2/time_cond_20.png", width="640">
        <img src="2/2.2/time_cond_epoch_20.gif", width="640">
        <figcaption>
            2.3 Sampling at epoch 5 and epoch 20.
        </figcaption>
    </figure>
    <p>
        Following the time conditioning, we also add class conditioning, where we feed in the class which we are training on (and during generation). The model handles 
        it in a similar way, except we also one-hot encode the class into a vector, as well as have an unconditional vector which is just a vector of 0s. This is handled 
        by masking the data with 0s. During the trainig process, we mask it 10% of the time. This also allows us to use CFG to generate the samples.
    </p>
    <figure>
        <img src="2/2.4/class_condition.png", width="640">
        <figcaption>
            2.4 The training loss curve.
        </figcaption>
    </figure>
    <figure>
        <img src="2/2.4/class_cond_5.png", width="640">
        <img src="2/2.4/class_cond_epoch_5.gif", width="640">
        <br>
        <img src="2/2.4/class_cond_20.png", width="640">
        <img src="2/2.4/class_cond_epoch_20.gif", width="640">
        <figcaption>
            2.5 Sampling at epoch 5 and epoch 20.
        </figcaption>
    </figure>
</body>
</html>